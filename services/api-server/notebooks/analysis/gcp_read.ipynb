{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491e0db-b6d6-480c-be42-5b1e16ddb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  A manager that facilitates reading and writing files to GCP Storage\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from typing import List, Dict, Callable, Tuple, Union\n",
    "from mypy_extensions import TypedDict\n",
    "\n",
    "from google.cloud import storage # type: ignore\n",
    "\n",
    "def get_path_prefix(root_dir: str, relative_path: str) -> str:\n",
    "    prefix = ''\n",
    "    if root_dir:\n",
    "        prefix = root_dir.rstrip('/') + '/'\n",
    "\n",
    "    if relative_path and relative_path != '/':\n",
    "        prefix = prefix + relative_path.strip('/') + '/'\n",
    "\n",
    "    return prefix\n",
    "\n",
    "class PathNode(TypedDict, total=False):\n",
    "    name: str\n",
    "    type: str\n",
    "    size: float\n",
    "        \n",
    "class GCPStorageManager(object):\n",
    "\n",
    "    def __init__(self, storage_details: Dict, verbose: bool) -> None:\n",
    "        self._bucket_name = storage_details.get('bucket')\n",
    "        self._root_dir = storage_details.get('root')\n",
    "        self.client = storage.Client()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _abs_path(self, rel_path: str) -> str:\n",
    "        if not self._root_dir:\n",
    "            return rel_path\n",
    "\n",
    "        return os.path.join(self._root_dir, rel_path)\n",
    "\n",
    "    def _build_current_url(self) -> str:\n",
    "        if self._root_dir:\n",
    "            return \"https://console.cloud.google.com/storage/browser/\" + self._bucket_name + \"/\" + self._root_dir\n",
    "        else:\n",
    "            return \"https://console.cloud.google.com/storage/browser/\" + self._bucket_name\n",
    "\n",
    "    def get_root_dir(self) -> str:\n",
    "        return self._root_dir\n",
    "\n",
    "    def get_storage_details(self) -> Dict:\n",
    "        return {\n",
    "            'provider': file_utils.ProviderList.GCP_STORAGE,\n",
    "            'bucket': self._bucket_name,\n",
    "            'root': self._root_dir\n",
    "        }\n",
    "\n",
    "    def get_sync_login_command(self, env_vars: Dict) -> List[str]:\n",
    "        return None\n",
    "\n",
    "    def get_sync_url(self, path: str) -> str:\n",
    "        if not path or path == '/':\n",
    "            abs_path = self._root_dir\n",
    "        else:\n",
    "            abs_path = self._abs_path(path)\n",
    "        return f'gs://{self._bucket_name}/{abs_path}'\n",
    "\n",
    "    def get_sync_command(self, src_dir: str, remote_path: str) -> Callable:\n",
    "\n",
    "        def sync_call() -> Tuple[int, str]:\n",
    "            cmd = ['gsutil', 'rsync', '-r', src_dir, self.get_sync_url(remote_path)]\n",
    "            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout, stderr = p.communicate()\n",
    "            err_code = 0\n",
    "            stderr_val = ''\n",
    "\n",
    "            if p.returncode != 0:\n",
    "                logging.error('Copy response is: {}'.format(stderr.decode('utf-8')))\n",
    "                stderr_val = stderr.decode('utf-8')\n",
    "                err_code = p.returncode\n",
    "\n",
    "            return err_code, stderr_val\n",
    "\n",
    "        return sync_call\n",
    "\n",
    "    def rm_file(self, relative_path: str) -> None:\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        path = self._abs_path(relative_path)\n",
    "        blob = bucket.blob(path)\n",
    "        logging.info('Deleting file at ' + path)\n",
    "        blob.delete()\n",
    "\n",
    "    def rm_dir(self, relative_path: str) -> None:\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        prefix = get_path_prefix(self._root_dir, relative_path)\n",
    "        blob = bucket.blob(prefix)\n",
    "        logging.info('Deleting files at ' + prefix)\n",
    "        blob.delete()\n",
    "\n",
    "    def read_content(self, path: str, throw_exception: bool, read_range: str = None, streaming: bool = False) -> bytes:\n",
    "        try:\n",
    "            path = self._abs_path(path)\n",
    "            bucket = self.client.bucket(self._bucket_name)\n",
    "            blob = bucket.blob(path)\n",
    "            result = blob.download_as_bytes()\n",
    "\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Downloading content from {self._build_current_url()}/{path}\")\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    #def gen_presigned_url(self, path: str) -> str:\n",
    "    #    # dask and other frameworks explicit s3 link rather than a byte stream or contet (e.g. s3://bucket/foo.csv)\n",
    "    #    return self._s3.generate_presigned_url('get_object',\n",
    "    #                                           Params={'Bucket': self._bucket_name, 'Key': self._abs_path(path)})\n",
    "\n",
    "    # Checks to see if the job directory exists.  No side-effects.\n",
    "    def check_dir_exists(self, path: str) -> bool:\n",
    "        # Create the prefix for this particular job.\n",
    "        exists = False\n",
    "        prefix = get_path_prefix(self._root_dir, path)\n",
    "        blobs = list(self.client.list_blobs(\n",
    "            self._bucket_name, prefix=prefix\n",
    "        ))\n",
    "        if len(blobs) > 0:\n",
    "            exists = True\n",
    "\n",
    "        return exists\n",
    "\n",
    "    def _download_content(self, remote_path: str) -> bytes:\n",
    "        remote_path = self._abs_path(remote_path)\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        blob = bucket.blob(remote_path)\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Downloading content from {self._build_current_url()}/{remote_path}\")\n",
    "\n",
    "        fileobj = BytesIO()\n",
    "        blob.download_to_file(fileobj)\n",
    "        return fileobj.getvalue()\n",
    "\n",
    "    def download_file(self, remote_path: str, file_name: str) -> None:\n",
    "        # Move references to large data items across folders\n",
    "        remote_path = self._abs_path(remote_path)\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        blob = bucket.blob(remote_path)\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Downloading file {self._build_current_url()}/{remote_path} to {file_name}\")\n",
    "\n",
    "        dirname = os.path.dirname(file_name)\n",
    "\n",
    "        if dirname:\n",
    "            # Only create a directory if it's not ''\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "        blob.download_to_filename(file_name)\n",
    "\n",
    "    def download_and_unzip(self, remote_path: str, local_dir: str) -> None:\n",
    "        zip_bytes = self._download_content(remote_path)\n",
    "        file_utils.unzip_into_dir(zip_bytes, local_dir)\n",
    "\n",
    "    def download_dir(self, remote_path: str, local_path: str) -> int:\n",
    "        nFiles = 0\n",
    "        if self.verbose:\n",
    "            logging.info(\"Downloading folder: \" + remote_path + \" to \" + local_path)\n",
    "\n",
    "        prefix = file_utils.get_path_prefix(self._root_dir, remote_path)\n",
    "        for blob in self.client.list_blobs(self._bucket_name, prefix=prefix):\n",
    "            if blob.name.endswith('/'):\n",
    "                continue\n",
    "\n",
    "            rel_path = os.path.relpath(blob.name, prefix)\n",
    "            dest_pathname = os.path.join(local_path, rel_path)\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "                os.makedirs(os.path.dirname(dest_pathname))\n",
    "\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Downloading file {blob.name} to {dest_pathname}\")\n",
    "\n",
    "            blob.download_to_filename(dest_pathname)  # Download\n",
    "            nFiles += 1\n",
    "\n",
    "        return nFiles\n",
    "\n",
    "    def upload_content(self, content: bytes, file_name: str) -> None:\n",
    "        # Uploads file content to a specific filename location\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Uploading content to {self._build_current_url()}/{file_name}\")\n",
    "\n",
    "        blob = bucket.blob(self._abs_path(file_name))\n",
    "        blob.upload_from_file(BytesIO(content))\n",
    "\n",
    "    def list_directory(self, path: str, with_size: bool = False) -> Dict:\n",
    "        prefix = get_path_prefix(self._root_dir, path)\n",
    "        bucket = self.client.bucket(self._bucket_name)\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        nodes = []\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('/'):\n",
    "                nodes.append(PathNode(name=os.path.basename(blob.name[:-1]), type='folder'))\n",
    "            else:\n",
    "                nodes.append(PathNode(name=os.path.basename(blob.name), type='file'))\n",
    "\n",
    "        return {\n",
    "            'nodes': nodes\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19077f-ab85-4246-955d-f00e6e0137a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "print(os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d97ba-91e8-41e1-b662-4f97a75810a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = GCPStorageManager({\n",
    "    'bucket': 'partnerships-data-reporting',\n",
    "    'root': ''\n",
    "}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12aa392-38d0-41ff-8703-ca837fa4f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.list_directory('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "directory_contents = manager.list_directory('')['nodes']\n",
    "directory_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1624c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ids_set = set([])\n",
    "\n",
    "for file_metadata in directory_contents:\n",
    "    file_name = file_metadata['name']\n",
    "    # Company locations file case.\n",
    "    if '_locations.csv' in file_name:\n",
    "        company_id = file_name.replace('_locations.csv', '')\n",
    "        company_ids_set.add(company_id)\n",
    "\n",
    "company_ids_count = len(list(company_ids_set))\n",
    "print(f'There are {company_ids_count} companies in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037dda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_location_ids_set = set([])\n",
    "\n",
    "for company_id in company_ids_set:\n",
    "    locations_csv_string = manager.read_content(f'{company_id}/{company_id}_locations.csv', throw_exception=True)\n",
    "    locations_io = StringIO(locations_csv_string.decode(\"utf-8\"))\n",
    "    locations_dataframe = pd.read_csv(locations_io)\n",
    "    location_ids = locations_dataframe['locationId'].tolist()\n",
    "    for location_id in location_ids:\n",
    "        company_location_ids_set.add((company_id, location_id))\n",
    "\n",
    "company_location_ids_count = len(list(company_location_ids_set))\n",
    "print(f'There are {company_location_ids_count} company locations in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (company_id, location_id, location_sales_dataframe)\n",
    "company_location_dataframes = []\n",
    "\n",
    "for index, company_location_tuple in enumerate(company_location_ids_set):\n",
    "    company_id, company_location_id = company_location_tuple\n",
    "    print(f'[{index + 1}] Downloading sales dataframe for location ({company_id}, {company_location_id})')\n",
    "\n",
    "    try:\n",
    "        location_sales_csv_string = manager.read_content(f'{company_id}/{company_location_id}/{company_location_id}_sales.csv', throw_exception=True)\n",
    "        location_sales_io = StringIO(location_sales_csv_string.decode(\"utf-8\"))\n",
    "        location_sales_dataframe = pd.read_csv(location_sales_io)\n",
    "        company_location_dataframes.append((company_id, company_location_id, location_sales_dataframe))\n",
    "    except:\n",
    "        print(f'[{index + 1}] An exception occurred for location ({company_id}, {company_location_id})')\n",
    "\n",
    "company_location_dataframes_count = len(list(company_location_dataframes))\n",
    "print(f'There are {company_location_dataframes_count} company location dataframes in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ca3a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for company_id, location_id, location_sales_dataframe in company_location_dataframes:\n",
    "    print(company_id, location_id, location_sales_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bb74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10deaa85-d6c3-4030-aac3-58cf661e6053",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "manager.read_content('4A5DE8Zj5gDtSmCbn/4A5DE8Zj5gDtSmCbn_customers.csv', throw_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c32ed-b4bb-4069-bacd-6d45f4ed432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.download_file('4A5DE8Zj5gDtSmCbn/4A5DE8Zj5gDtSmCbn_customers.csv', '4A5DE8Zj5gDtSmCbn_customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ebc5c-7043-4c04-9ceb-9acd5a1f803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.download_file('4A5DE8Zj5gDtSmCbn/4A5DE8Zj5gDtSmCbn_locations.csv', '4A5DE8Zj5gDtSmCbn_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af165774",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.download_file('4A5DE8Zj5gDtSmCbn/JYAZRDbztXRLwJ5r6/JYAZRDbztXRLwJ5r6_sales.csv', 'JYAZRDbztXRLwJ5r6_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d31eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9386b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38866d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_csv_string = manager.read_content('8fkcBjcmScuidkvn2/8fkcBjcmScuidkvn2_customers.csv', throw_exception=True)\n",
    "locations_csv_string = manager.read_content('8fkcBjcmScuidkvn2/8fkcBjcmScuidkvn2_locations.csv', throw_exception=True)\n",
    "sales_csv_string = manager.read_content('8fkcBjcmScuidkvn2/uf4FdoqBp8jKtAsGg/uf4FdoqBp8jKtAsGg_sales.csv', throw_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_io = StringIO(customers_csv_string.decode(\"utf-8\"))\n",
    "customers_dataframe = pd.read_csv(customers_io)\n",
    "customers_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_io = StringIO(locations_csv_string.decode(\"utf-8\"))\n",
    "locations_dataframe = pd.read_csv(locations_io)\n",
    "locations_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_io = StringIO(sales_csv_string.decode(\"utf-8\"))\n",
    "sales_dataframe = pd.read_csv(sales_io)\n",
    "sales_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671f64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
