{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar = pd.read_csv('../ar_reports/orig/kiva_ar_2020_02_25.csv')\n",
    "ar = pd.read_csv('../ar_reports/orig/kiva_ar_2019_12_31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.drop(columns='Unnamed: 16', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.columns = map(str.lower, ar.columns)\n",
    "ar.columns = ar.columns.str.replace(\" \", \"_\")\n",
    "ar.rename(columns={'1-30_days': '30_days', \n",
    "                   '31-60_days': '60_days', \n",
    "                   '61-90_days': '90_days', \n",
    "                   '91+_days': 'over_90_days'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.replace(\" \", 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[['current', '30_days', '60_days', '90_days', 'over_90_days']] = ar[['current', '30_days', '60_days', '90_days', 'over_90_days']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar[ar['ar_account'] == 'EAST OF EDEN CANNABIS COMPANY'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe where state license is null, and where it is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_null_license = ar[ar['state_license'].isnull()]\n",
    "ar_full_license = ar[~ar['state_license'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get list of unique account names from df of missing licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing = ar_null_license['ar_account'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through to create an id for missing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vals = len(ar_missing)\n",
    "array_list = []\n",
    "max_val = 0\n",
    "for i in range(num_vals):\n",
    "    array_list.append(\"assign_\" + str(max_val))\n",
    "    max_val += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put into array the array list that was created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_array = np.asarray(array_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match the unique missing accounts with the unqiue id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_miss_fill = pd.DataFrame({'ar_account': ar_missing, 'state_license': id_array})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop the state license from the main df (that has all missing rows, not just unique) to then be merged with the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_null_pd = ar_null_license.drop(columns='state_license')\n",
    "ar_fill = ar_null_pd.merge(pd_miss_fill, on='ar_account', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine the missing ones to the non-missing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_final = ar_full_license.append(ar_fill, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ar_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of iterations of each payor. should this be done at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_final.groupby('ar_account', as_index=False)['state_license'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_payor = ar_final.groupby('ar_account', as_index=False)['state_license'].count()\n",
    "count_payor.rename(columns={'state_license': 'count_payor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_final.rename(columns={'_original_amount_': 'orig_amt', '_balance_outstanding_': 'bal_out'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_final[ar_final['ar_account'] == 'EAST OF EDEN CANNABIS COMPANY'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_days_over = ar_final.groupby('ar_account', as_index=False)['days_over_due'].sum()\n",
    "sum_orig_amt_clean = ar_final.groupby('ar_account', as_index=False)['orig_amt'].sum()\n",
    "sum_bal_out_clean = ar_final.groupby('ar_account', as_index=False)['bal_out'].sum()\n",
    "sum_current_clean = ar_final.groupby('ar_account', as_index=False)['current'].sum()\n",
    "sum_30_day_clean = ar_final.groupby('ar_account', as_index=False)['30_days'].sum()\n",
    "sum_60_day_clean = ar_final.groupby('ar_account', as_index=False)['60_days'].sum()\n",
    "sum_90_day_clean = ar_final.groupby('ar_account', as_index=False)['90_days'].sum()\n",
    "sum_over_90_day_clean = ar_final.groupby('ar_account', as_index=False)['over_90_days'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_payor.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [count_payor, sum_days_over, sum_orig_amt_clean, sum_bal_out_clean, sum_current_clean, sum_30_day_clean,\n",
    "               sum_60_day_clean, sum_90_day_clean, sum_over_90_day_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sums = reduce(lambda left,right: pd.merge(left,right,on=['ar_account'], how='outer'), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sums.rename(columns={'days_over_due': 'sum_days_over', \n",
    "                        'orig_amt': 'sum_orig_amt',\n",
    "                        'bal_out': 'sum_bal_out', \n",
    "                        'current': 'sum_current',\n",
    "                        '30_days': 'sum_30_day', \n",
    "                        '60_days': 'sum_60_day',\n",
    "                        '90_days': 'sum_90_day', \n",
    "                        'over_90_days': 'sum_over_90'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_final[['state_license', 'ar_account', 'company']].iloc[1448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_to_merge = ar_final[['state_license', 'ar_account', 'company']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_to_merge[ar_to_merge['state_license'] == 'C10-0000224-LIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_to_merge[['state_license', 'ar_account', 'company']].iloc[1448]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_to_merge[ar_to_merge.duplicated()]\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "ar_to_merge.drop_duplicates(subset='ar_account', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_to_merge[ar_to_merge['ar_account'] == 'EAST OF EDEN CANNABIS COMPANY'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sums = pd.merge(df_sums, ar_to_merge, on='ar_account', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sums[df_sums['ar_account'] == 'EAST OF EDEN CANNABIS COMPANY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if the balance outstanding is less than 500, then it is 0, then the whole row is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a copy to compare original vs edited at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sums['sum_orig_amt_v2'] = df_sums['sum_orig_amt']\n",
    "df_sums['sum_bal_out_v2'] = df_sums['sum_bal_out']\n",
    "df_sums['sum_current_v2'] = df_sums['sum_current']\n",
    "df_sums['sum_30_day_v2'] = df_sums['sum_30_day']\n",
    "df_sums['sum_60_day_v2'] = df_sums['sum_60_day']\n",
    "df_sums['sum_90_day_v2'] = df_sums['sum_90_day']\n",
    "df_sums['sum_over_90_v2'] = df_sums['sum_over_90']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find index of all the instances where the balance outstanding is less than 0 (includes negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = df_sums[df_sums['sum_bal_out_v2'] < 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "df_sums['sum_orig_amt_v2'].loc[list2] = 0\n",
    "df_sums['sum_bal_out_v2'].loc[list2] = 0\n",
    "df_sums['sum_current_v2'].loc[list2] = 0\n",
    "df_sums['sum_30_day_v2'].loc[list2] = 0\n",
    "df_sums['sum_60_day_v2'].loc[list2] = 0\n",
    "df_sums['sum_90_day_v2'].loc[list2] = 0\n",
    "df_sums['sum_over_90_v2'].loc[list2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if the original amount is 0 and the balance outstanding is over 500, then mark as \"Delinquent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change to write off or credit loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1 = df_sums[(df_sums['sum_orig_amt'] == 0) & (df_sums['sum_bal_out'] > 0)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sums['delinquent'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# df_sums['delinquent'].loc[list1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sums[df_sums['delinquent'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two steps above have removed all negatives from original / outstanding and if one is 0 then it's either delinquent or they are both 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply negatives to the positive in the days past due"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looping through columns going right to left to remove negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_sums)):\n",
    "    if df_sums['sum_over_90_v2'].iloc[x] < 0:\n",
    "        val1 = df_sums['sum_over_90_v2'].iloc[x] + df_sums['sum_90_day_v2'].iloc[x]\n",
    "        if val1 < 0:\n",
    "            val2 = df_sums['sum_60_day_v2'].iloc[x] + val1\n",
    "            if val2 < 0:\n",
    "                val3 = df_sums['sum_30_day_v2'].iloc[x] + val2\n",
    "                if val3 < 0:\n",
    "                    val4 = df_sums['sum_current_v2'].iloc[x] + val3\n",
    "                    df_sums['sum_over_90_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_current_v2'].iloc[x] = val4\n",
    "                else:\n",
    "                    df_sums['sum_over_90_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_30_day_v2'].iloc[x] = val3\n",
    "            else:\n",
    "                df_sums['sum_over_90_v2'].iloc[x] = 0\n",
    "                df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_60_day_v2'].iloc[x] = val2\n",
    "        else:\n",
    "            df_sums['sum_over_90_v2'].iloc[x] = 0\n",
    "            df_sums['sum_90_day_v2'].iloc[x] = val1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_sums)):\n",
    "    if df_sums['sum_90_day_v2'].iloc[x] < 0:\n",
    "        val1 = df_sums['sum_90_day_v2'].iloc[x] + df_sums['sum_60_day_v2'].iloc[x]\n",
    "        if val1 < 0:\n",
    "            val2 = df_sums['sum_30_day_v2'].iloc[x] + val1\n",
    "            if val2 < 0:\n",
    "                val3 = df_sums['sum_current_v2'].iloc[x] + val2\n",
    "                df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_current_v2'].iloc[x] = val3\n",
    "            else:\n",
    "                df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_30_day_v2'].iloc[x] = val2\n",
    "        else:\n",
    "            df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "            df_sums['sum_60_day_v2'].iloc[x] = val1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_sums)):\n",
    "    if df_sums['sum_60_day_v2'].iloc[x] < 0:\n",
    "        val1 = df_sums['sum_60_day_v2'].iloc[x] + df_sums['sum_30_day_v2'].iloc[x]\n",
    "        if val1 < 0:\n",
    "            val2 = df_sums['sum_current_v2'].iloc[x] + val1\n",
    "            df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "            df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "            df_sums['sum_current_v2'].iloc[x] = val2\n",
    "        else:\n",
    "            df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "            df_sums['sum_30_day_v2'].iloc[x] = val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_sums)):\n",
    "    if df_sums['sum_30_day_v2'].iloc[x] < 0:\n",
    "        val1 = df_sums['sum_30_day_v2'].iloc[x] + df_sums['sum_current_v2'].iloc[x]\n",
    "        df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "        df_sums['sum_current_v2'].iloc[x] = val1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looping through left to right. all negatives will have been moved to current from above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_sums)):\n",
    "    if df_sums['sum_current_v2'].iloc[x] < 0:\n",
    "        val1 = df_sums['sum_current_v2'].iloc[x] + df_sums['sum_30_day_v2'].iloc[x]\n",
    "        if val1 < 0:\n",
    "            val2 = df_sums['sum_60_day_v2'].iloc[x] + val1\n",
    "            if val2 < 0:\n",
    "                val3 = df_sums['sum_90_day_v2'].iloc[x] + val2\n",
    "                if val3 < 0:\n",
    "                    val4 = df_sums['sum_over_90_v2'].iloc[x] + val3\n",
    "                    df_sums['sum_current_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_90_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_over_90_v2'].iloc[x] = val4\n",
    "                else:\n",
    "                    df_sums['sum_current_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_60_day_v2'].iloc[x] = 0\n",
    "                    df_sums['sum_90_day_v2'].iloc[x] = val3\n",
    "            else:\n",
    "                df_sums['sum_current_v2'].iloc[x] = 0\n",
    "                df_sums['sum_30_day_v2'].iloc[x] = 0\n",
    "                df_sums['sum_60_day_v2'].iloc[x] = val2\n",
    "        else:\n",
    "            df_sums['sum_current_v2'].iloc[x] = 0\n",
    "            df_sums['sum_30_day_v2'].iloc[x] = val1\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test to see if there are still any negatives in the DPD columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sums.iloc[:,14:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df_sums.iloc[:,14:19].apply(lambda x: pd.Series([(x < 0).sum()]), axis=1)\n",
    "max(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sums.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_sums[[\n",
    "    'state_license', \n",
    "    'ar_account', \n",
    "    'company',     \n",
    "    'count_payor', \n",
    "    'sum_days_over',\n",
    "    'sum_orig_amt_v2',\n",
    "    'sum_bal_out_v2', \n",
    "    'sum_current_v2', \n",
    "    'sum_30_day_v2', \n",
    "    'sum_60_day_v2',\n",
    "    'sum_90_day_v2', \n",
    "    'sum_over_90_v2'\n",
    "#     'delinquent'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal is to match to a company id, and if there isn't one to create one or find it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date_file'] = pd.datetime(2019, 12, 31)\n",
    "df_final['client'] = 'Kiva'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns='company', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.rename(columns={'state_license': 'license_number', \n",
    "           'sum_orig_amt_v2': 'orig_amt', \n",
    "          'sum_bal_out_v2': 'bal_out', \n",
    "          'sum_current_v2': 'current', \n",
    "          'sum_30_day_v2': '30_days',\n",
    "           'sum_60_day_v2': '60_days', \n",
    "          'sum_90_day_v2': '90_days', \n",
    "          'sum_over_90_v2': 'over_90_days', \n",
    "           'date_file': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['summary'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['license_provided'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[['license_number', \n",
    "          'license_provided',\n",
    "          'ar_account', \n",
    "          'count_payor', \n",
    "          'sum_days_over',\n",
    "           'orig_amt', \n",
    "          'bal_out', \n",
    "          'current', \n",
    "          '30_days', \n",
    "          '60_days', \n",
    "          '90_days',\n",
    "           'over_90_days', \n",
    "#           'delinquent', \n",
    "          'date', \n",
    "          'client', \n",
    "          'summary'\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delinq = pd.read_csv('../ar_reports/orig/no_neg/old/kiva_delinq_12_31_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_delinq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['delinquent'] = df_final['ar_account'].isin(df_delinq['ar_account']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final[df_final['delinquent'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('../ar_reports/orig/no_neg/kiva_ar_no_neg_12_31_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_excel('../ar_reports/kiva_ar_edited_2020_03_25.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "                                 \"SERVER=bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com;\"\n",
    "                                 \"DATABASE=ca_cannabis;\"\n",
    "                                 \"UID=admin;\"\n",
    "                                 \"PWD=N19lrqxnurTUJLJT6GFe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_sql('ar_report', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnxn = pyodbc.connect('Trusted_Connection=yes',\n",
    "#                       server = 'DESKTOP-KA6KCMH\\SQLEXPRESS', \n",
    "#                       driver = '{ODBC Driver 17 for SQL Server}',\n",
    "#                       database = 'ca_cannabis_v3'\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine('mssql+pyodbc://DESKTOP-KA6KCMH\\SQLEXPRESS/ca_cannabis_v3?driver=ODBC Driver 17 for SQL Server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar['30_day_clean'] = ar['30_days'] \n",
    "\n",
    "# ar['30_day_clean'] = ar['30_day_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['30_day_clean'] = ar['30_day_clean'].str.replace(\")\", \"\")\n",
    "# ar['30_day_clean'] = ar['30_day_clean'].str.replace(\",\", \"\")\n",
    "# ar['30_day_clean'] = ar['30_day_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['30_day_clean'] = ar['30_day_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"30_day_clean\")\n",
    "# ar['30_day_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# ar['60_day_clean'] = ar['60_days'] \n",
    "\n",
    "# ar['60_day_clean'] = ar['60_day_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['60_day_clean'] = ar['60_day_clean'].str.replace(\")\", \"\")\n",
    "# ar['60_day_clean'] = ar['60_day_clean'].str.replace(\",\", \"\")\n",
    "# ar['60_day_clean'] = ar['60_day_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['60_day_clean'] = ar['60_day_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"60_day_clean\")\n",
    "# ar['60_day_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# ar['90_day_clean'] = ar['90_days'] \n",
    "\n",
    "# ar['90_day_clean'] = ar['90_day_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['90_day_clean'] = ar['90_day_clean'].str.replace(\")\", \"\")\n",
    "# ar['90_day_clean'] = ar['90_day_clean'].str.replace(\",\", \"\")\n",
    "# ar['90_day_clean'] = ar['90_day_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['90_day_clean'] = ar['90_day_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"90_day_clean\")\n",
    "# ar['90_day_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# ar['over_90_day_clean'] = ar['over_90_days'] \n",
    "\n",
    "# ar['over_90_day_clean'] = ar['over_90_day_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['over_90_day_clean'] = ar['over_90_day_clean'].str.replace(\")\", \"\")\n",
    "# ar['over_90_day_clean'] = ar['over_90_day_clean'].str.replace(\",\", \"\")\n",
    "# ar['over_90_day_clean'] = ar['over_90_day_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['over_90_day_clean'] = ar['over_90_day_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"over_90_day_clean\")\n",
    "# ar['over_90_day_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# ar['orig_amt_clean'] = ar['original_amount'] \n",
    "\n",
    "# ar['orig_amt_clean'] = ar['orig_amt_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['orig_amt_clean'] = ar['orig_amt_clean'].str.replace(\")\", \"\")\n",
    "# ar['orig_amt_clean'] = ar['orig_amt_clean'].str.replace(\",\", \"\")\n",
    "# ar['orig_amt_clean'] = ar['orig_amt_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['orig_amt_clean'] = ar['orig_amt_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"orig_amt_clean\")\n",
    "# ar['orig_amt_clean_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# ar['bal_out_clean'] = ar['balance_outstanding'] \n",
    "\n",
    "# ar['bal_out_clean'] = ar['bal_out_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['bal_out_clean'] = ar['bal_out_clean'].str.replace(\")\", \"\")\n",
    "# ar['bal_out_clean'] = ar['bal_out_clean'].str.replace(\",\", \"\")\n",
    "# ar['bal_out_clean'] = ar['bal_out_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['bal_out_clean'] = ar['bal_out_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"bal_out_clean\")\n",
    "# ar['bal_out_clean_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "# ar['current_clean'] = ar['current'] \n",
    "\n",
    "# ar['current_clean'] = ar['current_clean'].str.replace(\"(\", \"-\")\n",
    "# ar['current_clean'] = ar['current_clean'].str.replace(\")\", \"\")\n",
    "# ar['current_clean'] = ar['current_clean'].str.replace(\",\", \"\")\n",
    "# ar['current_clean'] = ar['current_clean'].str.replace(\"$\", \"\")\n",
    "\n",
    "# ar['current_clean'] = ar['current_clean'].astype(float)\n",
    "\n",
    "# v1 = ar.columns.get_loc(\"current_clean\")\n",
    "# ar['current_clean_binary'] = ar.iloc[:,v1].apply(lambda x: 1 if x > 0 else 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
