{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import xlsxwriter\n",
    "from datetime import datetime\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.min_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this will update date info for later in the code\n",
    "\n",
    "today_date = date.today()\n",
    "\n",
    "today_year = today_date.strftime('%Y')\n",
    "today_month = today_date.strftime('%m')\n",
    "today_day = today_date.strftime('%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for California Data\n",
    "ca_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'ca_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Colorado Data\n",
    "co_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'co_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Illinois Data\n",
    "il_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'il_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Massachusetts Data\n",
    "ma_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'ma_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Massachusetts Data\n",
    "mi_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'mi_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Nevada Data\n",
    "nv_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'nv_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Oregon Data\n",
    "or_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'or_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine for Washington Data\n",
    "wa_conn = pyodbc.connect(server = 'bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com', \n",
    "                      driver = '{ODBC Driver 17 for SQL Server}',\n",
    "                      database = 'wa_cannabis',\n",
    "                      UID = 'admin',\n",
    "                      PWD = 'N19lrqxnurTUJLJT6GFe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is for updating with just California Data\n",
    "\n",
    "# ca_roll = pd.read_sql(\"SELECT * FROM ca_roll\", ca_conn)\n",
    "\n",
    "# df_roll = ca_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for updating with a combined file of California and Colorado Data\n",
    "\n",
    "ca_roll = pd.read_sql(\"SELECT * FROM ca_roll\", ca_conn)\n",
    "co_roll = pd.read_sql(\"SELECT * FROM co_roll\", co_conn)\n",
    "il_roll = pd.read_sql(\"SELECT * FROM il_roll\", il_conn)\n",
    "ma_roll = pd.read_sql(\"SELECT * FROM ma_roll\", ma_conn)\n",
    "mi_roll = pd.read_sql(\"SELECT * FROM mi_roll\", mi_conn)\n",
    "nv_roll = pd.read_sql(\"SELECT * FROM nv_roll\", nv_conn)\n",
    "or_roll = pd.read_sql(\"SELECT * FROM or_roll\", or_conn)\n",
    "wa_roll = pd.read_sql(\"SELECT * FROM wa_roll\", wa_conn)\n",
    "\n",
    "df_roll = ca_roll.append(co_roll).append(il_roll).append(ma_roll).append(mi_roll).append(nv_roll).append(or_roll).append(wa_roll)\n",
    "len(df_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this when the code below won't fun\n",
    "## It will help you find the error\n",
    "# list_of_dfs = []\n",
    "# for file in filenames:\n",
    "#     print(file)\n",
    "#     list_of_dfs.append(pd.read_csv(file, encoding = 'unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = Path('../ar_reports/orig/no_neg').glob('*.csv')\n",
    "list_of_dfs = [pd.read_csv(file, encoding = 'unicode_escape') for file in filenames]\n",
    "\n",
    "for dataframe, file in zip(list_of_dfs, filenames):\n",
    "    Dataframe['file'] = file\n",
    "df_file = pd.concat(list_of_dfs, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is stuff Nolan has done to try to fix the nulls being read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = pd.DataFrame(columns={'license_number', 'license_provided', 'ar_account', 'count_payor',\n",
    "#        'sum_days_over', 'orig_amt', 'bal_out', 'current', '30_days', '60_days',\n",
    "#        '90_days', 'over_90_days', 'delinquent', 'date', 'client', 'summary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in os.listdir('../ar_reports/orig/no_neg'):\n",
    "#     if df[-3:] == 'csv':\n",
    "#         read_df = pd.read_csv(f\"../ar_reports/orig/no_neg/{df}\", encoding='unicode_escape')\n",
    "#         all_dfs = all_dfs.append(read_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = all_dfs.reset_index().drop(columns={'index'})\n",
    "# all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs['date'] = pd.to_datetime(all_dfs['date'], format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file['date'] = pd.to_datetime(df_file['date'], format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_file\n",
    "# df_ar = all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a quick error checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar[df_ar['license_number'].isnull()]['client'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar['license_number'] = df_ar['license_number'].replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(df_ar)):\n",
    "    if df_ar['license_number'][n] == '':\n",
    "        df_ar['license_number'][n] = df_ar['ï»¿license_number'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar[df_ar['license_number'] == '']['client'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_ar[df_ar['license_number'].isnull()]['client'].unique()) > 0:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_ar.drop(columns={'ï»¿license_number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roll[df_roll['license_number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar[df_ar['license_number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar['license_number'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar['license_number'].str.strip().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar[df_ar['license_number'].str.strip().isnull()]['license_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar['license_number'] = df_ar['license_number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why this is what breaks the code\n",
    "# df_ar['license_number'] = df_ar['license_number'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll = pd.merge(df_ar, df_roll, on='license_number', how='left')\n",
    "df_ar_roll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll[df_ar_roll['license_number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ar_roll[df_ar_roll['roll_up_id'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll_fill = df_ar_roll[~df_ar_roll['roll_up_id'].isnull()]\n",
    "len(df_ar_roll_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll_fill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll[df_ar_roll['license_number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to be exported on a weekly basis - 2 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Combined list of all AR reports. To be used for identifying license numbers for AR reports as well as to be used for macro in underwriting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ar_roll_fill.to_csv(f'../../../combined_ar_list/combined_lists_{today_year}{today_month}{today_day}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just adds a duplicate license column to the end of df_ar_roll_fill so that my (Nolan's) VBA Macro can run properly\n",
    "df_ar_roll['license_end'] = df_ar_roll['license_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll_fill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_for_sql = df_ar_roll_fill.reset_index().drop(columns={'index', 'summary', 'roll_up_id', 'company_roll_up'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(len(df_ar_for_sql)):\n",
    "#     if df_ar_for_sql['current'][n] < 0:\n",
    "#         df_ar_for_sql['bal_out'][n] -= df_ar_for_sql['current'][n]\n",
    "#         df_ar_for_sql['current'][n] = 0\n",
    "#     if df_ar_for_sql['30_days'][n] < 0:\n",
    "#         df_ar_for_sql['bal_out'][n] -= df_ar_for_sql['30_days'][n]\n",
    "#         df_ar_for_sql['30_days'][n] = 0\n",
    "#     if df_ar_for_sql['60_days'][n] < 0:\n",
    "#         df_ar_for_sql['bal_out'][n] -= df_ar_for_sql['60_days'][n]\n",
    "#         df_ar_for_sql['60_days'][n] = 0\n",
    "#     if df_ar_for_sql['90_days'][n] < 0:\n",
    "#         df_ar_for_sql['bal_out'][n] -= df_ar_for_sql['90_days'][n]\n",
    "#         df_ar_for_sql['90_days'][n] = 0\n",
    "#     if df_ar_for_sql['over_90_days'][n] < 0:\n",
    "#         df_ar_for_sql['bal_out'][n] -= df_ar_for_sql['over_90_days'][n]\n",
    "#         df_ar_for_sql['over_90_days'][n] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_ar_for_sql.to_sql('combined_lists_include_nulls', engine_con, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll[df_ar_roll['ar_account'].str.contains('Â')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This reads the non null file into the combined ar lists and the file with nulls into the files to use for ar reports\n",
    "# df_ar_roll_fill.to_csv(f'../../../combined_ar_list/combined_lists_{today_year}{today_month}{today_day}.csv', index=False)\n",
    "df_ar_roll.to_csv(f'../../../ar_reports/files to use for ar report license identification/combined_lists_ar_reports/combined_lists_include_nulls_{today_year}_{today_month}_{today_day}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this re-removes the newly added columnm, as I can't be bothered to check if it will mess up code later on\n",
    "df_ar_roll.drop(columns=['license_end'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Move the AR data to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the SQL connection to the database we use\n",
    "\n",
    "params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "                                 \"SERVER=bespoke-database-1.cmevrozrcs7c.us-west-2.rds.amazonaws.com;\"\n",
    "                                 \"DATABASE=ar_reports;\"\n",
    "                                 \"UID=admin;\"\n",
    "                                 \"PWD=N19lrqxnurTUJLJT6GFe\")\n",
    "engine_con = create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds the AR report source so we can more properly add the new data\n",
    "\n",
    "df_ar_roll['ar_report'] = ''\n",
    "\n",
    "for n in range(len(df_ar_roll)):\n",
    "    ar_year = str(df_ar_roll['date'][n].year)\n",
    "    ar_month = str(df_ar_roll['date'][n].month)\n",
    "    if len(ar_month) == 1:\n",
    "        ar_month = '0' + ar_month\n",
    "    ar_day = str(df_ar_roll['date'][n].day)\n",
    "    if len(ar_day) == 1:\n",
    "        ar_day = '0' + ar_day\n",
    "        \n",
    "    ar_client = df_ar_roll['client'][n].lower().replace(' ', '_')\n",
    "    \n",
    "    ar_file = f\"{ar_client}_{ar_year}_{ar_month}_{ar_day}\"\n",
    "    \n",
    "    df_ar_roll['ar_report'][n] = ar_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_roll.to_excel('df_ar_for_sql.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fills the new dataframe\n",
    "\n",
    "ar_file_info = pd.DataFrame(df_ar_roll['ar_report'].unique(), columns={'ar_report'})\n",
    "\n",
    "ar_file_info['client'] = ''\n",
    "\n",
    "for n in range(len(ar_file_info)):\n",
    "     ar_file_info['client'][n] = df_ar_roll[df_ar_roll['ar_report'] == ar_file_info['ar_report'][n]]['client'].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just reorders the columns\n",
    "ar_file_info = ar_file_info[['client', 'ar_report']]\n",
    "ar_file_info = ar_file_info.sort_values(by='ar_report').reset_index().drop(columns={'index'})\n",
    "ar_file_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ar_file_info = pd.read_sql('ar_files', con=engine_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ar_file_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ar_file_list = []\n",
    "\n",
    "for ar in ar_file_info['ar_report'].unique():\n",
    "    if ar not in old_ar_file_info['ar_report'].unique():\n",
    "        new_ar_file_list.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ar_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ar_list_df = pd.DataFrame(new_ar_file_list, columns={'ar_report'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ar_df = new_ar_list_df.merge(df_ar_roll, on='ar_report')\n",
    "new_ar_file_df = new_ar_list_df.merge(ar_file_info, on='ar_report')\n",
    "new_ar_file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop here so I can check that the code is doing what it is supposed to\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this adds the file dataframe to sql\n",
    "\n",
    "new_ar_df.to_sql('ar_data', con=engine_con, if_exists='append', index=False)\n",
    "new_ar_file_df.to_sql('ar_files', con=engine_con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is only needed when a new state is added\n",
    "# roll_data_to_add = df_ar_roll[['license_number', 'company_roll_up', 'roll_up_id']]\n",
    "# roll_data_to_add.drop_duplicates(inplace=True)\n",
    "# roll_data_to_add.to_excel('add_rolls.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Credit database stats summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_ar_roll_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal = df_all.groupby(by='client')['bal_out'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bal_last_month =  df_all[df_all.date.dt.month.astype(str) + \"-\" + df_all.date.dt.year.astype(str) == str(today_date.month) + \"-\" + str(today_date.year)].groupby(by='client')['bal_out'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_all.groupby('client', as_index=True)['license_number'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nunique = df_all.groupby('client', as_index=True)['license_number'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roll_nunique = df_all.groupby('client', as_index=True)['roll_up_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mess of code is to add all the month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a FataFrame of the groupby data\n",
    "df_list_of_dates = pd.DataFrame(df_all.groupby('client', as_index=True)['date'].unique())\n",
    "df_list_of_dates = df_list_of_dates.rename(columns={'date':'months'})\n",
    "\n",
    "# This makes sure they are properly sorted, because they apparently aren't by default\n",
    "for n in range(len(df_list_of_dates)):\n",
    "    df_list_of_dates.months[n].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be way less complicated than it is, but w/e\n",
    "# This is for that columns I thought was for balance this month\n",
    "most_recent_month = []\n",
    "for n in range(len(df_list_of_dates)):\n",
    "    most_recent_month.append(df_list_of_dates[n:n+1]['months'][-1][-1])\n",
    "df_most_recent_month = pd.DataFrame(index=df_list_of_dates.index, data=most_recent_month, columns=['most_recent_month'])\n",
    "sum_of_most_recent = []\n",
    "for n in range(len(df_most_recent_month)):\n",
    "    sum_of_most_recent.append(df_all[df_all.client == df_most_recent_month.index[n]][df_all.date == pd.to_datetime(df_most_recent_month.most_recent_month).astype(str)[n]]['bal_out'].sum())\n",
    "df_most_recent_month['sum_of_most_recent'] = sum_of_most_recent\n",
    "df_bal_last_month = df_most_recent_month.drop(columns=['most_recent_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a list of all of the lengths of all of the lists of months, both for finding the longest one and for a column\n",
    "len_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets us the maximum length, which will be the number of columns we will need to create for the months\n",
    "for n in range(len(df_list_of_dates)):\n",
    "    len_list.append(len(df_list_of_dates.months[n]))    \n",
    "num_cols = max(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a  the the # of Reports columns\n",
    "report_length = pd.DataFrame(index=df_list_of_dates.index, data=len_list, columns=['num_of_reports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates said number of month columns\n",
    "for i in range(num_cols):\n",
    "    df_list_of_dates[f\"month_{i+1}\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(df_list_of_dates)):\n",
    "    for i in range(len(df_list_of_dates.months[n])):\n",
    "        df_list_of_dates[f\"month_{i+1}\"][n] = df_list_of_dates.months[n][i]\n",
    "\n",
    "# This drops the old months and turns the rest into pd.datatime instead of np.datetime, which is a mess\n",
    "df_list_of_dates = df_list_of_dates.drop(columns='months').apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "# This fills the NaTs with blanks because I found the NaTs annoying\n",
    "# df_list_of_dates = df_list_of_dates.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roundabout way to do this, I know, but adding the index as it's onw column in surpringly complicated\n",
    "df_list_of_companys = df_list_of_dates.drop(axis=1, columns=list(df_list_of_dates.columns.unique()))\n",
    "df_list_of_companys['company'] = df_list_of_companys.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this is the final draft of what we want to send to Excel\n",
    "credit_stats = pd.concat([df_list_of_companys,\n",
    "                          df_count,\n",
    "                          df_nunique.rename(\"unique_license\"),\n",
    "                          df_roll_nunique.rename(\"unique_rolls\"),\n",
    "                          df_bal,\n",
    "                          df_bal_last_month.rename(columns={\"sum_of_most_recent\": \"total_rec_bal\"}),\n",
    "                          report_length, df_list_of_dates], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_stats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should clean the way the Datetime columns are read into Excel\n",
    "\n",
    "for col in list(credit_stats.columns):\n",
    "    if col[:5] == 'month':\n",
    "        credit_stats[col] = pd.to_datetime(credit_stats[col]).apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_stats.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_row = {'company': f\"{len(credit_stats.company.unique())} Companies\", \n",
    "           'license_number': credit_stats.license_number.sum(),\n",
    "           'unique_license': len(list(df_ar_roll_fill.license_number.unique())),\n",
    "           'unique_rolls': len(list(df_ar_roll_fill.roll_up_id.unique())),\n",
    "           'bal_out': credit_stats.bal_out.sum(),\n",
    "           'total_rec_bal': credit_stats.total_rec_bal.sum(),\n",
    "           'num_of_reports': credit_stats.num_of_reports.sum()\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_stats = credit_stats.append(final_row, ignore_index=True).set_index('company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_stats.to_excel(f\"../../../Credit Database Stats/Credit Database Stats {today_month}.{today_day}.{today_year}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_stats.to_excel(f\"../../../Credit Database Stats/Credit Database Stats {today_month}.{today_day}.{today_year}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
